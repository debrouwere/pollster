#!/usr/bin/env python

import sys
import os
import time
import json
import functools
from copy import copy
from urlparse import urlparse
from collections import namedtuple
import socialshares
from boto import dynamodb2, sqs
from boto.dynamodb2.table import Table
from boto.dynamodb2.exceptions import ConditionalCheckFailedException
from boto.ec2 import cloudwatch
import redisjobs as jobs


# unbuffered standard output
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)


def flatten(obj, skip=[], connector='_', parent_key=''):
    items = []

    for sub_key, v in obj.items():
        if parent_key:
            key = parent_key + connector + sub_key
        else:
            key = sub_key

        if isinstance(v, dict) and not (key in skip):
            items.extend(flatten(v, skip, connector, key).items())
        elif isinstance(v, list):
            items.append((key, v))
        else:
            items.append((key, v))

    return dict(items)

def to_sqs(obj, config):
    connection = sqs.connect_to_region(
        config.region, 
        aws_access_key_id=config.username,
        aws_secret_access_key=config.password, 
        )
    queue = connection.get_queue('social-shares')
    message = sqs.message.RawMessage()
    message.set_body(json.dumps(obj))
    queue.write(message)

def get_keys(obj):
    return dict(url=obj['url'], timestamp=obj['timestamp'])

def to_dynamodb(data, config):
    connection = dynamodb2.connect_to_region(
        config.region,
        aws_access_key_id=config.username,
        aws_secret_access_key=config.password,
        )
    table = Table('social-shares', connection=connection)

    # occassionally, a careful and frequent poller will try 
    # to put data into DynamoDB at the same time, leading
    # to a timestamp+url key clash
    try:
        table.put_item(data=data)
    except ConditionalCheckFailedException:
        obj = table.get_item(**get_keys(data))
        obj._data.update(data)
        obj.save()

def to_console(url, keys):
    platforms = ', '.join(keys)
    print 'Fetched {} counts for {}'.format(platforms, url)

def to_cloudwatch(keys, container, config):
    connection = cloudwatch.connect_to_region(
        config.region, 
        aws_access_key_id=config.username, 
        aws_secret_access_key=config.password, 
        )
    for key in keys:
        connection.put_metric_data('social-shares', 'polls', value=1, unit='Count', dimensions={
            'container': container, 
            'platform': key, 
            })

Credentials = namedtuple('Credentials', ['region', 'username', 'password'])

def env(key):
    values = urlparse(os.environ[key])
    region = values.hostname or os.environ.get('AWS_REGION')
    username = values.username or os.environ.get('AWS_ACCESS_KEY_ID')
    password = values.password or os.environ.get('AWS_SECRET_ACCESS_KEY')
    return Credentials(region, username, password)

def fetch(platforms, meta):
    url = meta['payload']
    counts = socialshares.fetch(url, platforms, attempts=1)
    platforms = counts.keys()
    item = copy(counts)
    item['url'] = url
    item['timestamp'] = int(time.time())
    to_sqs(flatten(item), env('POLLSTER_SQS'))
    to_dynamodb(flatten(item), env('POLLSTER_DYNAMODB'))
    to_cloudwatch(platforms, os.environ['CONTAINER'], env('POLLSTER_CLOUDWATCH'))
    to_console(url, platforms)

schedules = {
    'frequent': ['facebook', 'twitter'], 
    'careful': ['facebookfql', 'pinterest', 'linkedin', 'google'], 
}

if __name__ == '__main__':
    board = jobs.Board(host=os.environ['JOBS_REDIS_HOST'])
    schedule = sys.argv[1]
    platforms = schedules[schedule]
    fetcher = functools.partial(fetch, platforms)
    print 'Commence responding to {} jobs'.format(schedule)
    board.respond('pollster/' + schedule, fetcher)
